{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3364939,"sourceType":"datasetVersion","datasetId":2029496},{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center><b>CNN for Alzheimer's Detection From MRI Images</b></center>","metadata":{}},{"cell_type":"markdown","source":"# INTRODUCTION\nAlzheimer’s disease (AD) is a neurological disorder that results in diminished cognitive function. AD onset most often occurs when people are in their mid 60s and is the most frequent cause of dementia in seniors. It is currently estimated that over 6 million American’s over 65 have AD [4]. There are currently no cures for AD, however there are some treatment strategies. Early detection and intervention have been shown to slow disease progression and improve the quality of life for individuals suffering from AD [3]. Definitively diagnosing AD while someone is alive remains a challenge for the medical community and several metrics need to be assessed to determine if an individual is suffering from AD [2]. These methods may include brain scans such as magnetic resonance imaging (MRI), cognitive assessments through testing of memory, attention and problem solving, overall health assessment, and examining environmental and biological factors [2]. Developing models that help detect early-stage AD would be a great help to those suffering from the disease. The focus of this project will be to build a convolutional neural network (CNN) to detect AD in MRI scans. ","metadata":{}},{"cell_type":"markdown","source":"# LIBRARIES","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np \nimport pandas as pd \nimport math\nfrom PIL import Image\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, BatchNormalization, Activation, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.activations import leaky_relu\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA EXPLORATION AND PROCESSING\nThe dataset used for this project consists of images from MRI brain scans. The MRI technique is non-invasive and can produce detailed images of soft tissue such as brain tissue [1]. Changes in brain structure such as cerebral atrophy (shrinking of the brain), and abnormal protein build up are characteristics of AD [2]. The data is comprised of four classes, Non-Demented, Mild Demented, Moderate Demented, and Very Mild Demented. The data is preprocessed so little cleaning needs to be done. However, the images will still be normalized using a data generator. There is a class imbalance in the data with only 64 images associated with moderate dementia. \n\n**Alzheimer MRI Preprocessed Dataset Available at Kaggle**\nhttps://www.kaggle.com/datasets/borhanitrash/alzheimer-mri-disease-classification-dataset/data\n","metadata":{}},{"cell_type":"markdown","source":"## Directories","metadata":{}},{"cell_type":"code","source":"dataset_dir = \"/kaggle/input/imagesoasis/Data\" \n\n# Check if the directory exists \nif os.path.exists(\"/kaggle/input/imagesoasis\"):\n    print(\"Dataset directory exists\")\n    print(\"Contents:\", os.listdir(\"/kaggle/input/imagesoasis\"))\nelse:\n    print(\"Dataset directory not found\")\n    print(\"Available inputs:\", os.listdir(\"/kaggle/input\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check Categories","metadata":{}},{"cell_type":"code","source":"categories = os.listdir(dataset_dir)\nprint(\"Categories:\", categories)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Data\n\n**Check Number of Images per Category**","metadata":{}},{"cell_type":"code","source":"image_paths = []\nlabels = []\n\nfor class_idx, class_name in enumerate(categories):\n    class_path = os.path.join(dataset_dir, class_name)\n        \n    # Get image files \n    files = glob.glob(os.path.join(class_path, \"*.jpg\"))\n    if not files:\n        files = glob.glob(os.path.join(class_path, \"*.jpeg\"))\n    if not files:\n        files = glob.glob(os.path.join(class_path, \"*.png\"))\n    \n    print(f\"Category: {class_name}, Files found: {len(files)}\")\n    \n    for file_path in files:\n        image_paths.append(file_path)\n        labels.append(class_idx)\n\nprint(f\"Total images found: {len(image_paths)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Generator\nNormalize the pixel values of the images by dividing them by 255 ensuring that the pixel values are in the range [0, 1] instead of [0, 255]. Also, splits the data into testing and validation sets. ","metadata":{}},{"cell_type":"code","source":"# Create ImageDataGenerator objects\ndatagen = ImageDataGenerator(validation_split=0.25, rescale=1./255)\n\n# Training data generator\ntrain_generator = datagen.flow_from_directory(\n    dataset_dir,\n    target_size=(256, 256), # Resize images to 256x256 pixels\n    batch_size=32,\n    class_mode='categorical', # For multi-class classification\n    subset='training'\n)\n\n# Validation data generator\nvalidation_generator = datagen.flow_from_directory(\n    dataset_dir,\n    target_size=(256, 256), # Resize images to 256x256 pixels\n    batch_size=32,\n    class_mode='categorical', # For multi-class classification\n    subset='validation'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display Example Images","metadata":{}},{"cell_type":"code","source":"classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n\nnum_images_display = 2\n\n# Initialize plot\nfig, axes = plt.subplots(num_images_display, len(classes), figsize = (len(classes) * 3, num_images_display * 3))\n\n# Loop to get images from each class\nfor i, class_name in enumerate(classes):\n    class_dir = os.path.join(dataset_dir, class_name)\n    images = os.listdir(class_dir)[:num_images_display]\n    \n    for j, img_name in enumerate(images):\n        img_path = os.path.join(class_dir, img_name)\n        img = image.load_img(img_path, target_size =(256, 256))\n        img_array = image.img_to_array(img)/ 255.0\n        \n        ax = axes[j,i]\n        ax.imshow(img_array)\n        ax.axis('off')\n        ax.set_title(f\"{class_name}\")\n        \nplt.tight_layout\n# Save image (optional)\n#plt.savefig('example_mri.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODELS\nCNNs will be used to detect AD in brain tissue by analyzing MRI images. CNNs are an appropriate choice for this task for several reasons. For one they use convolutional layers with local receptive fields to recognize patterns such as edges, textures and shape. Given the changes in brain structure associated with AD [2], detecting patterns such as these could be helpful in diagnosing the disease. CNNs process images through multiple layers, they learn to extract increasingly complex features. Early layers detect simple structures, while deeper layers can capture more complex patterns, such as those associated with cerebral atrophy and protein build-up. ","metadata":{}},{"cell_type":"markdown","source":"### Data Augmentation\nData augmentation helps prevent overfitting by generating diverse training samples. This is done by randomly rotating, flipping, zooming or adjusting the contrast of some images. Defining the augmentation this way will allow for it to be easily take in and out of models if required.","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.15, fill_mode='nearest'),\n    tf.keras.layers.RandomZoom(0.15, fill_mode='nearest'),\n    tf.keras.layers.RandomTranslation(0.1, 0.1, fill_mode='nearest'), # slight translation\n    tf.keras.layers.RandomContrast(0.2),\n    tf.keras.layers.RandomBrightness(0.1),\n    #tf.keras.layers.GaussianNoise(0.01), # adds a small amount of blur\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Learning Rate Scheduler\nThis scheduler will be used to decrease the learning rate during model training. ","metadata":{}},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', # assess validation loss\n    factor = 0.5, # amount to reduce lr by\n    patience = 2, # number of epochs to wait with no improvemnet \n    min_lr = 1e-6, # min lr not to go below\n    verbose = 1 # print out when lr is reduce\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Performance Plot Function\nPlots the accuracy, loss and auc scores from model training history.","metadata":{}},{"cell_type":"code","source":"def model_performance_plots(history, model_name):\n    \"\"\"\n    Plot the accuracy, loss, and auc score from the training history.\n    Inputs: \n    history - the history of the fit method of the model\n    model_name - the name of the model\n    \"\"\"\n\n    plt.figure(figsize=(15, 5))\n\n    # Accuracy plot\n    plt.subplot(1, 3, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title(f'{model_name} Accuracy')\n    plt.legend(loc='lower right')\n\n    # Loss plot\n    plt.subplot(1, 3, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(f'{model_name} Loss')\n    plt.legend(loc='upper right')\n\n    # AUC Score plot\n    plt.subplot(1, 3, 3)\n    plt.plot(history.history['auc'], label='Train AUC')\n    plt.plot(history.history['val_auc'], label='Validation AUC')\n    plt.xlabel('Epoch')\n    plt.ylabel('AUC')\n    plt.title(f'{model_name} AUC')\n    plt.legend(loc='lower right')\n\n    plt.tight_layout()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Architecture\n\n**Convolutional Layers:** capture spatial hierarchies in the data\n\nConv2D(64, (3, 3), padding='same', activation='leaky_relu', input_shape=(256, 256, 3)): The first convolutional layer with 64 filters of size 3x3, using ReLU activation, with padding to keep the dimensions the same as the input. The input shape is 256x256 with 3 channels (RGB).\n\nConv2D(128, (3, 3), activation='leaky_relu',padding='same'): The second convolutional layer with 128 filters. \n\nConv2D(256, (3, 3), activation='leaky_relu',padding='same'): The third convolutional layer with 256 filters. \n\nConv2D(512, (3, 3), activation='leaky_relu',padding='same'): The third convolutional layer with 512 filters. \n\n**Dropout:** \n\nDropout icreasing: Randomly sets designated percentage of the input units to 0 during training, which helps prevent overfitting.\n\nDropout(0.3): Applied before the dense layers, with a higher dropout rate to prevent overfitting as the model becomes more complex.\n\n\n**Batch Normalization:**\n\nAdded after each convolutional layer to normalize the activations, stabilize the learning process, and potentially reduce overfitting.\n\n**Max Pooling:**\n\nMaxPooling2D((2, 2)) reduces the spatial dimensions by half, and helps the model focus on the most important features.\n\n**GlobalAveragePooling:** \n\nGlobalAveragePooling before dense layers reduces parameters to 512 inputs.\n\n**Dense Layers:**\n\nDense(256): A fully connected layer with 256 units. Larger dense layer can learn more complex and abstract features from the data allowing the model to capture more detailed information and interactions.\n\nDense(4, activation='softmax'): The output layer with 4 units (since it’s a 4-class classification problem) and softmax activation to output class probabilities.\n","metadata":{}},{"cell_type":"code","source":"# Define input\ninputs = Input(shape=(256, 256, 3))\nx = inputs\n# Add data augmentation layer that's only active during training\nx = data_augmentation(x, training=True)\n\nx = Conv2D(64, (3, 3), padding='same', activation=leaky_relu, kernel_regularizer=l2(0.001))(x)\nx = Dropout(0.05)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n\nx = Conv2D(128, (3, 3), padding='same', activation=leaky_relu, kernel_regularizer=l2(0.001))(x)\nx = Dropout(0.1)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n\nx = Conv2D(256, (3, 3), padding='same', activation=leaky_relu, kernel_regularizer=l2(0.001))(x)\nx = Dropout(0.15)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n\nx = Conv2D(512, (3, 3), padding='same', activation=leaky_relu, kernel_regularizer=l2(0.001))(x)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2, 2))(x)\n\n# Global Average Pooling to reduce spatial dimensions\nx = GlobalAveragePooling2D()(x)\n\n# Fully connected layers\nx = Dense(256)(x)\nx = Dropout(0.4)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n# Output \noutputs = Dense(4, activation='softmax', kernel_regularizer=l2(0.001))(x)\n\n# Build model\nmodel = Model(inputs, outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Class Weight Calcuation\nThe dataset is severely imbalanced, and class weights will be implemented to help with this. ","metadata":{}},{"cell_type":"code","source":"\"\"\"\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_generator.classes),  # Use training data\n    y=train_generator.classes\n)\nclass_weights = dict(enumerate(class_weights))\nprint(\"Class weights:\", class_weights)\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above class weight calculation resulted in the model collapsing into a single class, 'Non Demented'. Therefore, a customized class weight calculation will be implemented. ","metadata":{}},{"cell_type":"code","source":"class_counts = {\n    0: 5002,  # Mild Demented\n    1: 488,   # Moderate Demented\n    2: 67222, # Non Demented\n    3: 13725  # Very Mild Demented\n}\n\ntotal = sum(class_counts.values())\nclass_weights = {cls: total/ (len(class_counts) * count) for cls, count in class_counts.items()}\n\nmax_weight = 15\nmin_weight = 1.0\nfor cls in class_weights:\n    class_weights[cls] = np.clip(class_weights[cls], min_weight, max_weight)\n\nprint(\"Class Weights\", class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.00005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy', 'auc'])\n\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit model\nhistory = model.fit(\n    train_generator,\n    validation_data = validation_generator,\n    epochs=15,\n    callbacks=[reduce_lr],\n    class_weight=class_weights\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"# Plot results\nmodel_plots = model_performance_plots(history, \"CNN Alzheimer's Performance\")\nplt.savefig(\"/kaggle/working/model_performance_plots.jpg\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# True labels from the validation generator\nvalidation_labels = validation_generator.classes\n\n# Predict the probabilities for the validation data\npredictions = model.predict(validation_generator)\n\n# Convert the probabilities to labels\npredicted_classes = np.argmax(predictions, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate confusion matrix\nconf_matrix = confusion_matrix(validation_labels, predicted_classes)\n\n# Plot \nplt.figure(figsize=(8, 8), dpi=100)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=validation_generator.class_indices.keys(), yticklabels=validation_generator.class_indices.keys())\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.xticks(rotation = 45, ha = 'right')\nplt.title('Confusion Matrix')\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/model_confusion_matrix.jpg\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONCLUSION\nAdult dementia is devastating to the individuals inflicted and their loved ones. If current population trends continue, there will be a dramatic increase in the number of people suffering from AD [5]. Deep learning techniques such as CNN could be helpful in detecting early-stage AD and directing treatments to those afflicted sooner. \n\n**Improvements**\n\nDealing with overfitting while increasing accuracy needs to be addressed. One possible way to improve the model’s performance would be to apply transfer learning. Foster showed that using a pretrained MobileNetV2 architecture to detect AD in MRI images enhanced a deep learning model [3]. However, this was on a binary AD classification not a multi-class problem. Transfer learning could be further enhanced by implementing autotune, which helps tune hyperparameters of the pre-trained model. Continuing to tune this model could be helpful as well. \n","metadata":{}},{"cell_type":"markdown","source":"# REFERENCES\n\n[1] Ashby, K., Adams, B. N., & Shetty, M. (2022, November 14). Appropriate magnetic resonance imaging ordering. StatPearls - NCBI Bookshelf. https://www.ncbi.nlm.nih.gov/books/NBK565857/\n\n[2] Coupé, P., Manjón, J. V., Lanuza, E., & Catheline, G. (2019). Lifespan changes of the human brain in Alzheimer’s disease. Scientific Reports, 9(1). https://doi.org/10.1038/s41598-019-39809-8\n\n[3] Foster, L. (2023, April 18). Identifying Alzheimer’s Disease with Deep Learning: A Transfer Learning Approach. Medium. https://medium.com/@lfoster49203/identifying-alzheimers-disease-with-deep-learning-a-transfer-learning-approach-620abf802631\n\n[4] “How Is Alzheimer’s Disease Diagnosed?”. National Institute on Aging. Dec.08, 2022. https://www.nia.nih.gov/health/alzheimers-symptoms-and-diagnosis/how-alzheimers-disease-diagnosed\n\n[5] Rasmussen, J., & Langerman, H. (2019). Alzheimer’s Disease – Why We Need Early    Diagnosis. Degenerative Neurological and Neuromuscular Disease, Volume 9, 123–130. https://doi.org/10.2147/dnnd.s228939\n\n[6] “What Is Alzheimer’s Disease?”. National Institute on Aging, Jul. 08, 2021. https://www.nia.nih.gov/health/alzheimers-and-dementia/what-alzheimers-disease\n\n\nAlzheimer MRI Preprocessed Dataset Available at Kaggle:\nhttps://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset\n","metadata":{}}]}